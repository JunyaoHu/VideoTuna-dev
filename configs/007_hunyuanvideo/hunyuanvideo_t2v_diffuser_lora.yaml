model:
  base_learning_rate: 6e-6
  target: videotuna.models.hunyuan.hyvideo_t2v.hunyuanvideo.HunyuanVideoWorkFlow
  params: 
    # VAE of HunyuanVideo
    first_stage_config:
      target: diffusers.AutoencoderKLHunyuanVideo
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "vae"
        load_dtype: fp16 # bf16 5b / fp16 2B 
        # load_dtype: fp32 # bf16 5b / fp16 2B 
    
    # Text encoder 
    cond_stage_config:
      target: transformers.LlamaModel
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "text_encoder"
        # torch_dtype: auto # bf16 5b / fp16 2B 
        torch_dtype: float16 # bf16 5b / fp16 2B 

    tokenizer_config: 
      target: transformers.LlamaTokenizerFast
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "tokenizer"
        # torch_dtype: auto # bf16 5b / fp16 2B 
        # torch_dtype: fp16 # bf16 5b / fp16 2B 
        torch_dtype: float16 # bf16 5b / fp16 2B 


    cond_stage_config_2:
      target: transformers.CLIPTextModel
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "text_encoder_2"
        # torch_dtype: auto # bf16 5b / fp16 2B 
        # torch_dtype: fp16 # bf16 5b / fp16 2B 
        torch_dtype: float16 # bf16 5b / fp16 2B 
    
    tokenizer_config_2: 
      target: transformers.CLIPTokenizer
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "tokenizer_2"
        # torch_dtype: auto # bf16 5b / fp16 2B 
        # torch_dtype: fp16 # bf16 5b / fp16 2B 
        torch_dtype: float16 # bf16 5b / fp16 2B 


    # Denosier model
    denoiser_config:
      target: diffusers.HunyuanVideoTransformer3DModel
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: "transformer"
        load_dtype: fp16 # bf16 5b / fp16 2B 
        # load_dtype: fp32 # bf16 5b / fp16 2B 
        
    
    # Lora module 
    adapter_config: 
      target: peft.LoraConfig
      params:
        r: 4
        lora_alpha: 1.0 
        init_lora_weights: True
        target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

    # # Diffusion sampling scheduler
    scheduler_config:
      target: diffusers.FlowMatchEulerDiscreteScheduler
      params:
        pretrained_model_name_or_path: checkpoints-hunyuan
        subfolder: scheduler
    # scheduler_config:
    #   target: diffusers.CogVideoXDPMScheduler
    #   params:
    #     pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b
    #     subfolder: scheduler


# data configs
data:
  target: videotuna.data.lightning_data.DataModuleFromConfig
  params:
    batch_size: 1
    num_workers: 16
    wrap: false
    train:
      target: videotuna.data.cogvideo_dataset.VideoDataset
      params:
        instance_data_root: "inputs/t2v/hunyuanvideo/tyler_swift_video"
        dataset_name: null 
        dataset_config_name: null
        caption_column: "labels.txt"
        video_column: "videos.txt"
        height: 256
        width: 256
        fps: 28
        max_num_frames: 17
        skip_frames_start: 0
        skip_frames_end: 0
        cache_dir: ~/.cache
        id_token: null

# training configs
lightning:
  strategy: fsdp
  trainer:
    benchmark: True
    num_nodes: 1
    accumulate_grad_batches: 2
    max_epochs: 2000
    # precision: 32
    precision: 16
  callbacks:
    image_logger:
      target: videotuna.utils.callbacks.ImageLogger
      params:
        batch_frequency: 100000
        max_images: 2
        to_local: True # save videos into local files
        log_images_kwargs:
          unconditional_guidance_scale: 6
    metrics_over_trainsteps_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        filename: "{epoch:06}-{step:09}"
        save_weights_only: False
        # every_n_epochs: 1
        every_n_train_steps: 20
    

